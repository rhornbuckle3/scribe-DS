\section{Random Forests}
A Random Forest can circumvent some of the issues faced by decision trees and somewhat mitigate the model's tendency to overfit.
\begin{enumerate}[(1)]
\item
 Randomly select \textit{n} Samples from \textbf{X} out of a maximum size of N (\textit{n}=~0.8N is a good size)
\item
Obtain a Decision Tree with those \textit{n} samples
\item
Repeat steps 1 \& 2 to obtain trees:  $T_1,\,T_2,\,T_3,\,T_4,...,\, T_\textit{t}$
\item
Given a new sample, \textit{x}, "classify" it according to $T_1(\textit{x}),\,T_2(\textit{x}),\,T_3(\textit{x}),\,...,\,T_\textit{t}(\textit{x})$. We'll call it T(\textit{x}).
\item
Final Decision: $$T(\textit{x})=\frac{1}{\textit{t}}\sum_{i=1}^{\textit{t}} T_i(\textit{x})$$
\end{enumerate}
This can also be stated as: \qquad T(\textit{x})=consensus of ($T_1(\textit{x}),...,\,T_\textit{t}(\textit{x}))$





\begin{siderules}
\begin{myExample}[Practical Example: Infidelity]
Is my girlfriend/boyfriend/whatever (my \textit{dog}?) cheating on me?
\begin{center}
\begin{tabular}{| l || c | c | c | c | c | c | c | c | c | c | c | c | c |}
\hline
\textbf{X}=\textbf{Variables} & 1 & 2 & 3 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14  \\ \hline \hline
Age & 25 & 20 & 33 & 23 & 25 & 32 & 27 & 21 & 25 & 27 & 26 & 29 & 47  \\ \hline
Length & $\frac{1}{2}$ & 2 & 7 & 5 & 6 & 10 & 3 & 4 & $\frac{3}{2}$ & 3 & 4 & $\frac{1}{3}$ & 1 \\ \hline
\# of children & 0 & 0 & 2 & 1 & 0 & 3 & 0 & 0 & 0 & 0 & 1 & 0 & 6 \\ \hline
\# of hours away & 20 & 7 & 10 & 7 & 13 & 2 & 8 & 7 & 13 & 11 & 8 & 4 & 12 \\ \hline
\# of trips per year & 2 & 3 & 5 & 2 & 2 & 5 & 10 & 4 & 3 & 2 & 0 & 2 & 6 \\ \hline
Previous offenses & 2 & 1 & 0 & 2 & 0 & 0 & 2 & 3 & 0 & 4 & 0 & 0 & 13 \\ \hline \hline 
\textbf{Y}=Cheating (?) & 1 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 1 & 1 & 1\\ \hline 
\end{tabular}
\end{center}
We will compute this Random Forest next lecture.
\end{myExample}
\end{siderules}
